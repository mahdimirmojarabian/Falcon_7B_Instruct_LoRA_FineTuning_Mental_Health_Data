{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 🚀 **Fine-Tuning Falcon-7B-Instruct with LoRA on a Mental Health Conversational Dataset**\n","\n","* This notebook demonstrates the fine-tuning of the **Falcon-7B-Instruct Sharded Model** on a mental health conversational dataset curated by **heliosbrahma**.  \n","\n","* Both the model and dataset are available on **Hugging Face**, and links are provided in the notebook.\n","\n","---"],"metadata":{"id":"MuWvHMGIOoWr"}},{"cell_type":"markdown","source":["## 📌 **1. Setup Dependencies and Imports**\n","First, we will install the required dependencies, configure the Hugging Face login, and prepare the environment for model training."],"metadata":{"id":"LOWAQZuH6hHu"}},{"cell_type":"code","source":["#all installs\n","!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n","!pip install -q datasets bitsandbytes einops wandb\n","!pip install huggingface_hub\n","\n","#all imports\n","import torch\n","import time\n","from huggingface_hub import notebook_login\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n","from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"-bys4q4u45v-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 🔗 **2. Connect to Hugging Face Hub**\n","We'll authenticate with Hugging Face to access the datasets and models."],"metadata":{"id":"8GIOWuRZ6Ylc"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"e8WvDjEH5DNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"RGmfFB7y51vP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 📥 **3. Load the Dataset**\n","We will load the mental health conversational dataset directly from **Hugging Face Datasets**."],"metadata":{"id":"83DM_0Jh6UGU"}},{"cell_type":"code","source":["dataset_name = \"heliosbrahma/mental_health_chatbot_dataset\"\n","data = load_dataset(dataset_name)\n","data"],"metadata":{"id":"YavG6n_v53kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 💡 **4. Model Loading and Configuration**\n","We are using the **Falcon-7B-Instruct Sharded Model**.  \n","> **Sharding**: This technique splits the model into smaller chunks, allowing for parallel processing across multiple devices—ideal for memory-constrained environments.  "],"metadata":{"id":"sYyr3zgC6s2v"}},{"cell_type":"code","source":["model_name = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","model.config.use_cache = False"],"metadata":{"id":"z_UF5i8y6uuX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 🔄 **5. Load the Tokenizer**\n","We load the tokenizer for text processing, which is essential for converting text data into model-compatible formats."],"metadata":{"id":"sQpti-BT7iRM"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"X9tc6alQ7el2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 🔧 **6. Configure LoRA (Low-Rank Adaptation)**\n","We set up the **LoRA configuration** for parameter-efficient fine-tuning.  \n","Below are the main parameters:  \n","\n","- **lora_rank**: Defines the low-rank dimensions. Smaller ranks result in fewer trainable parameters (e.g., 512x32 and 32x512 in adapter layers).  \n","- **lora_alpha**: Scaling factor that adjusts the magnitude of LoRA layers.  \n","- **lora_dropout**: Dropout rate to prevent overfitting during training.  \n","- **bias**: Specifies if bias parameters should be trained (`none`, `all`, or `lora_only`).  \n","- **task_type**: Since this is a text generation task, it is set to `CAUSAL_LM`.  \n","- **target_modules**: Specifies the model components (like attention blocks) where LoRA updates are applied."],"metadata":{"id":"jaD6T0827oq_"}},{"cell_type":"code","source":["model = prepare_model_for_kbit_training(model)\n","\n","lora_alpha = 32 #16\n","lora_dropout = 0.05 #0.1\n","lora_rank = 32 #64\n","\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_rank,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\n","        \"query_key_value\",\n","        \"dense\",\n","        \"dense_h_to_4h\",\n","        \"dense_4h_to_h\",\n","    ]\n",")\n","\n","peft_model = get_peft_model(model, peft_config)"],"metadata":{"id":"yra-1a5Y7mEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 🏋️ **7. Initialize the Trainer**\n","We initialize the **SFTT Trainer** with appropriate parameters for fine-tuning.  "],"metadata":{"id":"s7fKxNMl7yxq"}},{"cell_type":"code","source":["output_dir = \"falcon7binstruct_mentalhealthmodel_oct23\"\n","per_device_train_batch_size = 16 #4\n","gradient_accumulation_steps = 4\n","optim = \"paged_adamw_32bit\"\n","save_steps = 10\n","logging_steps = 10\n","learning_rate = 2e-4\n","max_grad_norm = 0.3\n","max_steps = 180 #100 #500\n","warmup_ratio = 0.03\n","lr_scheduler_type = \"cosine\" #\"constant\"\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    fp16=True,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=True,\n","    lr_scheduler_type=lr_scheduler_type,\n","    push_to_hub=True\n",")"],"metadata":{"id":"lx275FLR73F1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## ⚙️ **8. Configure Training Arguments (Passing arguments to the SFTT trainer)**\n","We provide necessary training arguments to the trainer for optimal performance and stability."],"metadata":{"id":"pJjV_ZaY77-A"}},{"cell_type":"code","source":["max_seq_length = 256\n","\n","trainer = SFTTrainer(\n","    model=peft_model,\n","    train_dataset=data['train'],\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")"],"metadata":{"id":"2TU8j6K277QI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### For more stable training"],"metadata":{"id":"HS0J9SrluY81"}},{"cell_type":"code","source":["# upcasting the layer norms in torch.bfloat16 for more stable training\n","for name, module in trainer.model.named_modules():\n","    if \"norm\" in name:\n","        module = module.to(torch.bfloat16)"],"metadata":{"id":"LfSd78Oa8Lk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 🚀 **9. Model Training**\n","Finally, we train the model using the defined configurations.  \n","You can monitor your training in **Weights & Biases** for real-time metrics.  \n","\n","> 💡 **Tip**: If you are using **Google Colab**, prevent disconnection with the following snippet:  \n","> Open the console (`Ctrl+Shift+I` → Console tab) and paste:  \n","```javascript\n","function ClickConnect(){\n","    console.log(\"Working\");\n","    document.querySelector(\"colab-toolbar-button\").click();\n","}\n","setInterval(ClickConnect, 60000);\n","```"],"metadata":{"id":"AagnspEt8Ih3"}},{"cell_type":"code","source":["start = time.time()"],"metadata":{"id":"yIvpPo0w8aEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_model.config.use_cache = False\n","trainer.train()"],"metadata":{"id":"m3EjW2Vh8oz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["end=time.time()"],"metadata":{"id":"UF0ou_828pXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_taken=end-start\n","print(time_taken)"],"metadata":{"id":"OdkHjzOI8rwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 💾 **10. Save the Fine-Tuned Model**\n","Save the fine-tuned model to your local or cloud storage for future use."],"metadata":{"id":"pfLbcEIN8vcF"}},{"cell_type":"code","source":["#trainer.save() # if you want to save your model locally"],"metadata":{"id":"rWBNQRZ78wwj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## ☁️ **11. Push to Hugging Face Hub**\n","Upload the trained model to **Hugging Face Hub** to share and reuse."],"metadata":{"id":"2s2XlYZ983_6"}},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"tz2zXTiL81r7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> 📊 **Check Training Metrics**: Visit your **Weights & Biases** account for detailed insights into training performance."],"metadata":{"id":"a_UkWO0UxNCy"}},{"cell_type":"markdown","source":["---\n","\n","## 🔍 **12. Inference**\n","For inference, you will need to:  \n","1. **Clear memory or restart the kernel:** For inference from your original model and the fine-tuned model, you need to clean your memory or restart your kernel. Remember to push the fine-tuned model to Hugging Face so that you can pull your new model.  \n","2. **Load both models:** Load your original model as before, then load your fine-tuned model. This wraps the base model with your PEFT model.\n"],"metadata":{"id":"YjDjJTJA8-nV"}},{"cell_type":"code","source":["# Loading PEFT model\n","PEFT_MODEL = \"Srishy/falcon7binstruct_mentalhealthmodel_oct23\"\n","config = PeftConfig.from_pretrained(PEFT_MODEL)\n","peft_base_model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    return_dict=True,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n","\n","peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","peft_tokenizer.pad_token = peft_tokenizer.eos_token"],"metadata":{"id":"O75HchDaIbxa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 💡 **13. Model Response Comparison**\n","Finally, let's see how the responses differ between the **original model** and the **fine-tuned model** on the same input prompts."],"metadata":{"id":"7_b8sPTJu3hG"}},{"cell_type":"code","source":["# Generate responses from both orignal model and fine-tuned model\n","def get_response(question):\n","  prompt = f\"\"\"\n","  ###Instruction: You are a mental health professional, answer the following question correctly.\n","  If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'\n","\n","  ###Question: {question}\n","\n","  ###Response:\n","\n","  \"\"\"\n","\n","  encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","  outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = tokenizer.eos_token_id, \\\n","                                                                                                                     eos_token_id = tokenizer.eos_token_id, attention_mask = encoding.attention_mask, \\\n","                                                                                                                     temperature=0.1, top_p=0.1, repetition_penalty=1.2, num_return_sequences=1,))\n","  text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","  #print(dashline)\n","  print(f'Response from original falcon_7b_instruct_sharded:\\n{text_output}')\n","\n","  print(\"*******************************************************\")\n","\n","  peft_encoding = peft_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n","                                                                                                                      eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n","                                                                                                                      temperature=0.1, top_p=0.1, repetition_penalty=1.2, num_return_sequences=1,))\n","  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n","\n","  print(f'Response from fine-tuned falcon_7b_instruct_sharded:\\n{peft_text_output}')\n"],"metadata":{"id":"zyHt2-Bh8_qO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_response(\"Are there cures for mental health problems?\")"],"metadata":{"id":"VplatacNG384"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_response(\"What’s the difference between psychotherapy and counselling?\")"],"metadata":{"id":"JQPwlMF7G9L2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_response(\"What happens in a therapy session?\")"],"metadata":{"id":"Gmu7zle-LR0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_response(\"Are neurofeedback and biofeedback the same thing?\")"],"metadata":{"id":"8xpQh31kLX6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_response(\"What is insomnia disorder?\")"],"metadata":{"id":"15h3K4M5Ngi7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### ✅ **Conclusion**\n","\n","You have successfully fine-tuned the **Falcon-7B-Instruct** model on a mental health conversational dataset using **LoRA**. The fine-tuned model is now ready for deployment or further experimentation.\n"],"metadata":{"id":"P9_uObt5_dho"}}]}